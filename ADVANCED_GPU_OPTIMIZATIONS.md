# Advanced GPU Optimization Analysis and Implementation Plan

## Executive Summary

After analyzing similar GPU-accelerated cryptocurrency applications (BTCRecover, Vanitygen++, BitCrack, cuda-bitcoin-address-lab) and the current entropy-lab-rs implementation, this document outlines advanced optimizations to achieve 3-6x additional performance gains beyond the current 2-4x improvements.

## Current State Assessment

### âœ… Already Implemented (OPENCL_OPTIMIZATIONS.md)
1. **Device-Aware Work Group Sizing** - 10-30% gain
2. **Pinned Memory Allocation** - 20-50% gain  
3. **Aggressive Compiler Optimizations** - 5-15% gain
4. **Memory Access Coalescing** - 10-40% gain
5. **Compute Unit Occupancy** - 15-25% gain
6. **Constant Memory** - Algorithm constants (k_sha256, k_sha512, secp256k1 precomputed points)

### Current Performance: 2-4x over baseline

## Benchmarking Against Similar Applications

### Performance Comparison Table

| Application | Technology | Crypto Operations | Reported Performance | Key Optimizations |
|------------|-----------|------------------|---------------------|-------------------|
| BTCRecover | OpenCL/Python | BIP39, SHA-256, PBKDF2 | 14-100x vs CPU | End-to-end GPU, local memory, workgroup tuning |
| Vanitygen++ | OpenCL/CUDA | secp256k1, SHA-256, RIPEMD-160 | 68M keys/sec (P5000) | Vector types, kernel fusion, async transfers |
| BitCrack | CUDA/OpenCL | Full address generation | 10-50M keys/sec | Optimized batch sizing, minimal CPU-GPU transfers |
| cuda-bitcoin-address-lab | CUDA | Bitcoin address formats | ~100M hashes/sec | Kernel fusion, shared memory, constant memory |

### Key Insights from Similar Apps
1. **Local Memory Critical**: Hash workspaces in local memory (20-40% gain)
2. **Vector Operations**: uint4/uint8 for parallel ops (10-25% gain)
3. **Kernel Fusion**: Combine operations to reduce launches (10-20% gain)
4. **Async Transfers**: Overlap compute and memory (15-30% gain)
5. **End-to-End GPU**: Minimize CPU-GPU round trips (major gain)

## Advanced Optimization Opportunities

### Phase 1: Local Memory for Hash Operations (Priority: HIGH)

**Target Files**: `cl/sha2.cl`, `cl/sha512.cl`, `cl/batch_address.cl`

**Optimization**: Store SHA-256/SHA-512 working blocks in local memory shared across workgroup.

**Implementation**:
```c
__kernel void batch_address_with_local(
    __global const ulong * restrict entropies_hi,
    __global const ulong * restrict entropies_lo,
    __global uchar * restrict output_addresses,
    uint purpose,
    __local uint * restrict local_workspace  // NEW: shared workspace
) {
    const uint local_id = get_local_id(0);
    const uint group_id = get_group_id(0);
    const uint group_size = get_local_size(0);
    
    // Allocate per-thread space in local memory
    __local uint *my_workspace = &local_workspace[local_id * 64]; // 64 uints = 256 bytes
    
    // Use local workspace for SHA-256/SHA-512 operations
    // ... rest of kernel
}
```

**Expected Gain**: 20-40% (based on BTCRecover published results and OpenCL best practices)

**Note**: These performance estimates are based on:
- BTCRecover's documented GPU acceleration guide showing 14-100x gains with similar techniques
- OpenCL optimization literature indicating 20-40% gains from local memory for hash operations
- Actual gains will vary by GPU vendor, driver version, and workload characteristics
- Validation on target hardware is required to confirm these projections

**Rationale**: 
- Local memory is 10-100x faster than global memory
- Hash operations perform many repeated reads/writes to working state
- Workgroup can share memory bandwidth efficiently

### Phase 2: Vector Type Operations (Priority: HIGH)

**Target Files**: `cl/sha2.cl`, `cl/secp256k1_field.cl`

**Optimization**: Use vector types (uint4, uint8) for SIMD operations in hash rounds.

**Implementation**:
```c
// Current: Scalar operations
unsigned int w0_t = W[0];
unsigned int w1_t = W[1];
unsigned int w2_t = W[2];
unsigned int w3_t = W[3];

// Optimized: Vector operations
uint4 w_vec0 = vload4(0, W);  // Load 4 words at once
uint4 w_vec1 = vload4(1, W);
// Process 4 rounds simultaneously
```

**Expected Gain**: 10-25% (vendor hardware dependent)

**Rationale**:
- Modern GPUs have SIMD execution units
- Vector operations reduce instruction count
- Better register utilization

### Phase 3: Kernel Fusion (Priority: MEDIUM)

**Target Files**: `src/scans/gpu_solver.rs`, new fused kernel in `cl/`

**Optimization**: Combine multiple kernel launches into single kernels.

**Current Flow** (multiple launches):
1. Generate mnemonic from entropy
2. PBKDF2 (2048 iterations)
3. Derive keys
4. Generate address

**Optimized Flow** (single launch):
- All operations in one kernel
- No intermediate GPU-CPU-GPU transfers
- Reduced kernel launch overhead

**Expected Gain**: 10-20%

**Rationale**:
- Each kernel launch has ~10-50Î¼s overhead
- Intermediate results stay in registers/local memory
- Reduces global memory traffic

### Phase 4: Asynchronous Transfers with Dual Queues (Priority: MEDIUM)

**Target Files**: `src/scans/gpu_solver.rs`

**Optimization**: Overlap computation with data transfers using multiple command queues.

**Implementation**:
```rust
pub struct GpuSolver {
    pro_que: ProQue,
    compute_queue: Queue,  // NEW: dedicated compute queue
    transfer_queue: Queue, // NEW: dedicated transfer queue
    // ...
}

impl GpuSolver {
    pub fn compute_batch_async(&self, ...) -> ocl::Result<Vec<[u8; 25]>> {
        // Start transfer of next batch on transfer_queue
        // While compute_queue is processing current batch
        // Pipeline: Transfer(N+1) || Compute(N) || Transfer_back(N-1)
    }
}
```

**Expected Gain**: 15-30% for large batch workloads

**Rationale**:
- Hide memory transfer latency behind computation
- Proven technique in BitCrack and Vanitygen++
- Most effective for batch processing scenarios

### Phase 5: Workgroup Barrier Optimization (Priority: LOW)

**Target Files**: All kernel files using local memory

**Optimization**: Minimize barrier() calls and optimize synchronization patterns.

**Implementation**:
- Analyze data dependencies
- Use barrier() only when truly needed
- Group operations to reduce sync points

**Expected Gain**: 5-10%

**Rationale**:
- Barriers stall all threads in workgroup
- Unnecessary barriers waste cycles
- Careful analysis can eliminate redundant syncs

### Phase 6: Register Pressure Optimization (Priority: LOW)

**Target Files**: `cl/batch_address.cl`, complex kernels

**Optimization**: Reduce register usage per thread to increase occupancy.

**Techniques**:
- Reduce number of live variables
- Reuse variables when safe
- Use local memory for spills instead of registers

**Expected Gain**: 5-15% (architecture dependent)

**Rationale**:
- Limited registers per compute unit
- High register usage reduces active wavefronts
- More wavefronts = better latency hiding

## CUDA vs OpenCL Decision

### Analysis
Based on research and project requirements:

**OpenCL Advantages** (Current choice âœ…):
- Cross-platform: NVIDIA, AMD, Intel GPUs
- Rust ecosystem: mature `ocl` crate
- Sufficient performance for use case
- Wider deployment compatibility

**CUDA Advantages**:
- 5-10% better performance on NVIDIA GPUs
- Better tooling (Nsight Compute, NSight Systems)
- More optimized libraries (cuBLAS, cuFFT)
- Tighter hardware integration

**Recommendation**: **Stick with OpenCL**

**Rationale**:
1. Project targets security researchers with diverse hardware
2. OpenCL achieves 90-95% of CUDA performance when optimized
3. Adding CUDA would require dual codebase (2x maintenance)
4. OpenCL optimizations apply to all vendors
5. No CUDA-specific features needed for these algorithms

**Optional**: Consider CUDA as future enhancement if NVIDIA-specific deployment is critical, but NOT for initial optimization work.

## Implementation Roadmap

### Timeline and Risk Considerations

**Important Notes**:
- Estimates below assume experienced GPU developer familiar with OpenCL
- Hardware-specific issues often arise during testing phase
- Each phase includes time for validation, debugging, and optimization
- Risk buffers (20-30%) should be added for production timelines
- Dependencies: Phase 1 should complete before Phase 2-3 begin

**Critical Path**: Phase 1 â†’ Phase 4 â†’ Phase 2 â†’ Phase 3 â†’ Phase 5-6

### Week 1-2: Local Memory Optimization
- [ ] Implement local memory for SHA-256 working blocks
- [ ] Implement local memory for SHA-512 working blocks  
- [ ] Add local memory to batch_address kernel
- [ ] Benchmark and validate correctness
- [ ] **Target**: 20-40% gain, 2.4-5.6x total

### Week 3-4: Vector Type Operations
- [ ] Convert SHA-256 rounds to use uint4/uint8 vectors
- [ ] Convert secp256k1 field ops to use vectors
- [ ] Optimize memory alignment for vectors
- [ ] Benchmark across NVIDIA/AMD
- [ ] **Target**: 10-25% gain, 2.64-7x total

### Week 5-6: Kernel Fusion
- [ ] Design fused kernel architecture
- [ ] Implement fused batch_address_complete kernel
- [ ] Optimize register usage in fused kernel
- [ ] Validate against separate kernels
- [ ] **Target**: 10-20% gain, 2.9-8.4x total

### Week 7-8: Async Transfers
- [ ] Implement dual command queue system
- [ ] Add async batch processing API
- [ ] Pipeline multiple batches
- [ ] Measure overlap effectiveness
- [ ] **Target**: 15-30% gain, 3.3-10.9x total

### Week 9-10: Polish and Validation
- [ ] Fine-tune all optimizations
- [ ] Comprehensive benchmarking suite
- [ ] Test on NVIDIA, AMD, Intel GPUs
- [ ] Performance profiling with vendor tools
- [ ] Documentation updates

## Performance Testing Methodology

### Benchmark Suite Requirements

1. **Hardware Coverage**:
   - NVIDIA: GTX 1080, RTX 2060, RTX 3090
   - AMD: RX 580, RX 6800
   - Intel: Iris Xe

2. **Metrics to Track**:
   - Throughput (addresses/second)
   - Memory bandwidth utilization
   - GPU compute utilization
   - Kernel execution time
   - Data transfer time

3. **Test Cases**:
   - Small batches (100-1000)
   - Medium batches (10K-100K)
   - Large batches (1M+)
   - Different derivation paths
   - All scanner types

4. **Profiling Tools**:
   - NVIDIA: Nsight Compute, Nsight Systems
   - AMD: CodeXL, Radeon GPU Profiler
   - Intel: VTune Profiler
   - OpenCL: Built-in profiling API

### Validation Requirements

1. **Correctness**:
   - All test vectors pass
   - Output matches CPU reference
   - No precision loss in critical operations
   - Cryptographic operations verified

2. **Performance Regression**:
   - New optimizations don't hurt existing workloads
   - Small batch performance maintained
   - Memory usage reasonable

3. **Portability**:
   - Works on all supported GPUs
   - Graceful degradation if features unavailable
   - Fallback to previous implementation if needed

## Expected Final Results

### Performance Target Matrix

| Optimization Level | Throughput Gain | Total Speedup | Status |
|-------------------|----------------|---------------|---------|
| Baseline | 1x | 1x | âœ… |
| Current (Phase 0) | 2-4x | 2-4x | âœ… |
| + Local Memory | 1.2-1.4x | 2.4-5.6x | ðŸ”„ Planned |
| + Vector Types | 1.1-1.25x | 2.64-7x | ðŸ”„ Planned |
| + Kernel Fusion | 1.1-1.2x | 2.9-8.4x | ðŸ”„ Planned |
| + Async Transfer | 1.15-1.3x | 3.3-10.9x | ðŸ”„ Planned |
| **Final Target** | **3-11x** | **3-11x total** | ðŸŽ¯ Goal |

**Conservative Target**: 3-6x additional gain (5-24x total)
**Optimistic Target**: 6-11x additional gain (12-44x total)

### Real-World Impact

**Example: Cake Wallet RPC Scanner**
- Current: ~52 minutes (1,000 addr/sec CPU)
- After Phase 0: ~5-10 minutes (10,000 addr/sec GPU)
- After All Phases: ~1-2 minutes (50,000+ addr/sec GPU)

**Example: Profanity Vanity Address Search**
- Current: ~1M addresses/sec
- After All Phases: ~5-10M addresses/sec
- Competitive with dedicated vanity tools

## Risk Assessment and Mitigation

### Technical Risks

**Risk 1**: Local memory size limitations on some GPUs
- **Mitigation**: Query device capabilities, adjust allocation dynamically
- **Fallback**: Use global memory if local unavailable

**Risk 2**: Vector operations not supported on older GPUs
- **Mitigation**: Compile-time or runtime feature detection
- **Fallback**: Keep scalar code path

**Risk 3**: Kernel fusion increases register pressure
- **Mitigation**: Careful register usage analysis, spill to local memory
- **Fallback**: Keep separate kernel option

**Risk 4**: Async transfers don't overlap effectively
- **Mitigation**: Profile with vendor tools, adjust batch sizes
- **Fallback**: Synchronous mode still available

### Project Risks

**Risk 1**: Optimization time extends beyond budget
- **Mitigation**: Prioritize high-value optimizations first
- **Success Criteria**: Each phase shows measurable gain

**Risk 2**: Maintenance complexity increases
- **Mitigation**: Comprehensive documentation, automated tests
- **Code Review**: All changes reviewed for clarity

**Risk 3**: Portability issues across GPU vendors
- **Mitigation**: Test on multiple platforms early
- **CI/CD**: Automated testing on different hardware

## Conclusion

This advanced optimization plan builds on the solid foundation of existing optimizations (2-4x gain) to achieve an additional 3-6x performance improvement, bringing total gains to 6-24x over baseline CPU performance.

The optimizations are informed by proven techniques from similar GPU-accelerated cryptocurrency applications (BTCRecover, Vanitygen++, BitCrack) and OpenCL best practices.

**Key Decision**: Continue with OpenCL for cross-platform compatibility. CUDA would provide marginal gains but significantly limit deployment options.

**Implementation Strategy**: Phased approach with validation at each step ensures we don't introduce regressions and can measure actual gains per optimization.

**Success Criteria**: 
- âœ… 3x minimum additional performance gain
- âœ… All correctness tests pass
- âœ… Works on NVIDIA, AMD, Intel GPUs
- âœ… Comprehensive documentation
- âœ… No breaking changes to API

## References

1. BTCRecover GPU Acceleration Guide: https://docs.btcrecover.org/en/latest/GPU_Acceleration/
2. Vanitygen++ Docker: https://hub.docker.com/r/pihiker/oclvanitygen-gpu
3. BitCrack Optimization Guide: https://mizogg.com/learn/guides/bitcrack-optimization.html
4. CUDA Bitcoin Address Lab: https://github.com/MrF-crypto/cuda-bitcoin-address-lab
5. OpenCL Optimization Techniques: https://peerdh.com/blogs/programming-insights/opencl-performance-optimization-techniques
6. Khronos OpenCL Best Practices: https://www.khronos.org/opencl/
7. NVIDIA OpenCL Best Practices: https://developer.nvidia.com/opencl
8. AMD GPU Programming Guide: https://gpuopen.com/learn/
